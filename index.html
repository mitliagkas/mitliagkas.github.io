---
layout: default
---


<div class="home">

<!--   <table style="text-align: left; width: 90%;" border="0" cellpadding="2" cellspacing="2"> --> 

<table style="text-align: left;" border="0">
    <tbody>
      <tr>
        <td style="width=20%;vertical-align: top;">
        <!--img title="Istanbul, 2013", style="max-width:200px;" alt="That's me." src="ioannis-headshot-scaled.jpg"></td-->
        <img title="Ioannis", style="width:100%;" alt="That's me." src="ioannis-headshot-scaled.jpg">
	<br>
	<br>
        <img title="UdeM", style="width:50%;" alt="UdeM" src="images/logo-udem.png">
	<br>
	Assistant professor,
	Computer Science,
	University of Montr&eacute;al
	<br>
	<br>
	<a href="https://mila.quebec/" target="_blank">
        <img title="Mila", style="width:40%;" alt="Mila" src="images/logo-mila.png">
	<br>
	Core faculty at Mila
	<br>
	<br>
	</a>
	<a href="https://www.cifar.ca/ai/pan-canadian-artificial-intelligence-strategy/the-canada-cifar-ai-chairs" target="_blank">
        <img title="CIFAR", style="width:30%;" alt="CIFAR" src="images/logo-cifar.jpg">
	<br>
	Canada CIFAR AI Chair 
	</a>
	<br>
	<br>
	<br>

	<center>
	</center>
		<a href="cv.pdf" target="_blank">CV (October 2020)</a>
	<br>
	<br>


	<a href="mailto:ioannis@iro.umontreal.ca" target="_blank">Email,</a>
	<a href="https://scholar.google.com/citations?user=K757SxgAAAAJ&hl=en&oi=ao" target="_blank">Scholar,</a>
	<a href="https://twitter.com/bouzoukipunks" target="_blank">Twitter,</a>
	<a href="https://www.linkedin.com/in/ioannis-mitliagkas-6a8241131/" target="_blank">LinkedIn</a>


	</td>
        <td width=30px></td>
        <td style="vertical-align: top;">
	I work on topics in optimization, dynamics and learning, 
	with a focus on modern machine learning. 
	I have done work in the intersection of systems and theory.
	<!--I am interested in systems for modern machine learning and data analysis.-->
	Some recent topics:
	<ul>
		<li>Min-max optimization and the dynamics of games</li>
		<li>Generalization and domain adaptation</li>
		<li>Optimization for deep learning</li> 
		<li>Statistical learning and inference</li>
	</ul>

	<center>
		See my <a href="rs.pdf" target="_blank">research statement</a>
		for long-term vision 
		and  <a href="https://scholar.google.com/citations?hl=en&user=K757SxgAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">recent publications</a>
		for an idea of what I'm doing now
	</center> 
    <br>
    For two years in a row, I have co-organized the 
    <a href="https://sgo-workshop.github.io/">Smooth games optimization and ML</a> 
    workshop series at NeurIPS.
    The <a href="https://slideslive.com/38924034/opening-remarks?ref=speaker-22001-latest ">opening remarks</a> video from last year gives a nice summary of our motivation for this line of work. 

    <br>
    <br>

    In Montreal, I co-organize <a href="https://mtl-mlopt.github.io/">MTL MLOpt</a>,
    a bi-weekly meeting of optimization experts from Mila, UdeM, McGill (CS and math), Google Brain,
    SAIL, FAIR, MSR, etc. 
    <br>
    <br>

	Every fall, I teach <a href="/ift6390-ml-class">ML</a> to 200 grad students.
	Every winter, I teach an advanced research class on <a href="/ift6085-dl-theory-class-2020">deep learning theory</a>.

	<br>
	<br>



	Before joining the University of Montreal, I was a postdoc with the Departments of Computer Science and Statistics at Stanford University
        working with <a href="http://cs.stanford.edu/people/chrismre/" target="_blank">Chris R&eacute;</a> and <a href="http://web.stanford.edu/~lmackey/" target="_blank">Lester Mackey</a>.
	I got my PhD at The University of Texas at Austin with <a href="http://users.ece.utexas.edu/%7Ecmcaram/" target="_blank">Constantine Caramanis</a> and <a href="http://www.ece.utexas.edu/people/faculty/sriram-vishwanath" target="_blank">Sriram Vishwanath</a>,
	 where I also worked with <a href="http://users.ece.utexas.edu/~dimakis/" target="_blank">Alex Dimakis</a>.
	<br>
	<br>

	<font color="red" style="font-weight:bold">
		Prospective students: 
	</font>
	I am looking for particularly strong students, for MSc or PhD.
	Unfortunatley, I might not be able to respond to all emails.
	Please make sure to go over my  
	<a href="https://scholar.google.com/citations?hl=en&user=K757SxgAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">recent publications</a>
	and list of recent projects (below). 
	If you think that we have a strong overlap in interests and you have
<!--  already done research in the area feel free to email me. -->
	a strong background in mathematics and computation,
	please make sure to submit
	your <a href="https://mila.quebec/en/cours/admission/" target="_blank">supervision request</a> 
	 by December 15th (form opens in mid-October) and mention me as one of your faculty of choice.

	
	<br>


	</td>
        <!-- <img style="width: 200px; height: 300px;" alt="That's me." src="colourPortrait.jpg"></td>
         --><td style="vertical-align: top;">&nbsp;&nbsp; <br>
        </td>
      </tr>
    </tbody>
  </table>


<br>
<br>
<h2>Recent projects</h2>

<font color="red" style="font-weight:bold">
NEW: </font>
A curated list of some of my most exciting research projects. 
For a thorough list of projects grouped by topic, please consult the 
<a href="/projects">projects page</a>.
Publications listed below, in the present page.

<br>
<br>

<div id ="imagelist">
<ul>
      <li>
      <a href="https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update" target="_blank">
      <img src="{{ site.baseurl }}/images/project-bias-variance.png" alt="" title="A Modern Take on the Bias-Variance Tradeoff in Neural Networks">
      <div>
        A Modern Take on the Bias-Variance Tradeoff in Neural Networks
        <em>
	We measure prediction bias and variance in NNs.
	Both bias and variance decrease as the number of parameters
	grows. We decompose variance into
	variance due to sampling and variance due to initialization.
	</em>
	<br>
	Lead: Brady Neal
      </div></a>
      </li> 
      <li>
      <a href="https://arxiv.org/abs/1911.00804"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-adversarial-domain-generalization.png" alt="" title="Adversarial target-invariant representation learning for domain generalization">
      <div>
	Adversarial representation learning for domain generalization
        <em>
	We propose a process that enforces pair-wise domain invariance while training a feature extractor over a diverse set of domains.
	We show that this process ensures invariance to any distribution that can be expressed as a mixture of the training domains.
	</em>
	<br>
	Lead: Isabela Albuquerque, João Monteiro (INRS)
      </div></a>
      </li> 
      <li>
      <a href="https://arxiv.org/abs/2001.00602"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-games-spectral-shapes.png" alt="" title="Accelerating Smooth Games by Manipulating Spectral Shapes ">
      <div>
	Accelerating Smooth Games by Manipulating Spectral Shapes
        <em>
	We use matrix iteration theory to characterize acceleration in smooth games.
	The spectral shape of a family of games is the set containing all eigenvalues of the Jacobians of standard gradient dynamics in the family.
	</em>
	<br>
	Lead: Waiss Azizian
      </div></a>
      </li> 
      <li>
      <a href="http://papers.nips.cc/paper/8779-reducing-the-variance-in-online-optimization-by-transporting-past-gradients"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-igt.png" alt="" title="Reducing the variance in online optimization by transporting past gradients">
      <div>
        Reducing variance in online optimization by transporting past gradients
        <em>
        Implicit gradient transport turns past gradients into gradients evaluated at the current iterate.
        It reduces the variance in online optimization and can be used as a drop-in replacement for the gradient estimate in a number of well-understood methods such as heavy ball or Adam. 
    </em>
    <br>
    Lead: Sebastien Arnold
      </div></a>
      </li> 
      <li>
      <a href="https://openreview.net/forum?id=SkeMPEH32N"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-overparametrization-rl.png" alt="" title="In Support of Over-Parametrization in deep RL">
      <div>
	In Support of Over-Parametrization in deep RL
        <em>
	 There is significant recent evidence in supervised learning that, in the over-parametrized setting, wider networks achieve better test error.
	 We experiment on four OpenAI Gym tasks and provide evidence that overparametrization is also beneficial in deep RL.
	</em>
	<br>
	Lead: Brady Neal
      </div></a>
      </li> 
      <li>
      <a href="https://openreview.net/forum?id=SkeMPEH32N"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-mmc-gp.png" alt="" title="Connections between max margin classifiers and gradient penalty">
      <div>
	Connections between max margin classifiers and gradient penalties
        <em>
	 Maximum-margin classifiers can be formulated as Integral Probability Metrics (IPMs) or classifiers with some form of gradient norm penalty.
	 This implies a direct link to a class of Generative adversarial networks (GANs) which penalize a gradient norm.
	</em>
	<br>
	Lead: Alexia Jolicoeur-Martineau, 
	Image source: wikipedia
      </div></a>
      </li> 

      <li>
      <a href="https://arxiv.org/abs/1906.07300"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-games-lower-bounds.png" alt="" title="Linear Lower Bounds and Conditioning of Differentiable Games ">
      <div>
            Linear Lower Bounds and Conditioning of Differentiable Games
        <em>
		We approach the question of fundamental iteration complexity for smooth, differentiable games by providing lower bounds to complement the linear
		(i.e. geometric) upper bounds observed in the literature on a wide class of problems.
	</em>
	<br>
	Lead: Adam Ibrahim
      </div></a>
      </li> 
      <li>
      <a href="https://arxiv.org/pdf/2001.00602.pdf"  target="_blank">
      <img src="{{ site.baseurl }}/images/project-negative-momentum.png" alt="" title="Negative Momentum for Improved Game Dynamics">
      <div>
        Negative Momentum for Improved Game Dynamics
        <em>
	Alternating updates are more stable than
	simultaneous updates on simple games.
	A negative momentum
	term achieves convergence in a difficult toy adversarial problem, but also on the notoriously
	difficult to train saturating GANs
	</em>
	<br>
	Lead: Gauthier Gidel, Reyhane Askari-Hemmat
      </div></a>
      </li> 
    </ul>
</div>

  <br><br>

  <h2>Recent News</h2>
  <div id="" style="overflow-y:scroll; height:230px;">
  <ul>
		<li>September 2020: 
			Welcome to new PhD students, 
			<a href="https://ca.linkedin.com/in/ryan-d-orazio-05833979">Ryan D'Orazio</a> and 
			<a href="https://hiroki11x.github.io">Hiroki Naganuma</a>.
		<li>August 2020: Kartik Ahuja awarded the IVADO postdoctoral scholarship. Excited to have him join us in January 2021!
		<li>April 2020: Two papers on differentiable games 
			(<a href="https://arxiv.org/abs/2007.04202">one</a>, 
			<a href="https://arxiv.org/abs/1906.07300">two</a>)
			accepted at ICML'20.
		<li>January 2020: Two papers on efficient methods and tight bounds for differentiable games 
			(<a href="https://arxiv.org/pdf/2001.00602.pdf">one</a>, 
			<a href="https://arxiv.org/pdf/1906.05945.pdf">two</a>)
			accepted at AISTATS'20.
		<li>December 2019: Nicolas Loizou was awarded the IVADO postdoctoral scholarship at the prestigious Fellow tier.
		<li>December 2019: Brady Neal graduates with an MSc. He will continue on his PhD with us.
		<li>November 2019: Excited to be coorganizing the 2nd iteration of the <a href="https://sgo-workshop.github.io/">Smooth Games Optimization and Machine Learning Workshop</a> at NeurIPS'19.
		<li>November 2019: <a href="https://arxiv.org/abs/1906.03532">Reducing the variance in online optimization by transporting past gradients</a> selected for
		       	<span style="font-weight:bold">
			<font color="red">
				spotlight oral presentation	
			</font>
			</span> 
			at NeurIPS'19.
		<li>October 2019: Multiple submissions to AISTATS. Preprints on the way...
		<li>June 2019: At ICML with 3 papers in main conference, 2 in Deep Learning Phenomena workshop.
		<li>May 2019: <a href="https://arxiv.org/pdf/1905.11382.pdf">State-Reification Networks</a> selected for
		       	<span style="font-weight:bold">
			<font color="red">
				oral presentation	
			</font>
			</span> 
			at ICML'19.
		<li>April 2019: Excited to be <a href="https://i.redd.it/kovwct2jeix21.png">listed</a> among the most prolific authors of accepted ICML 2019 papers.
		<li>April 2019: I received the NSERC Discovery grant!
		<li>April 2019: In Japan for AISTATS.
		<li>January 2019: h-detach paper accepted at ICLR.
		<li>December 2018: Was nominated in the first cohort of Canada CIFAR AI chairs!!
		<li>December 2018: Co-organizing <a href="https://sgo-workshop.github.io/">Smooth Games Optimization in ML</a> workshop at NeurIPS.
		<li>December 2018: Negative momentum for improved game dynamics. Paper accepted at AISTATS 2019.
		<li>December 2018: Full version of YellowFin manscript accepted at SysML
		<li>September 2018: Excited to be teaching Machine Learning to a class of 180 graduate students at UdeM.
		<li>February 2018: <a href="https://arxiv.org/pdf/1706.03471.pdf">YellowFin</a> selected for
		       	<span style="font-weight:bold">
			<font color="red">
				oral presentation	
			</font>
			</span> 
			at SysML'18.
		<li>January 2018: Teaching new class! 
	      <a href="ift6085-dl-theory-class/">
		IFT 6085: Theoretical principles for deep learning
		</a>
		<li>December 2017: Accelerated power iteration via momentum, paper accepted at AISTATS 2018.
		<li>November 2017: Talk at Google Brain, Montréal
			<li>September 2017: Thrilled to be starting work at the University of Montreal and the Mila as an assistant professor!
		<li>August 2017: Visiting my alma mater, UT Austin.
		<li>August 2017: At Sydney for ICML, presenting work on YellowFin, custom scans for Gibbs sampling, and deep learning for 3D point cloud representation and generation.
		<li>July 2017: 
			<span style="font-weight:bold">
			<font color="red">
				New preprint!
			</font>
			</span> Representation Learning and Adversarial Generation of 3D Point Clouds
				<a href="https://arxiv.org/abs/1707.02392">[arxiv]</a>.
		<li>July 2017: 
			<span style="font-weight:bold">
			<font color="red">
				New preprint!
			</font>
			</span> Accelerated stochastic power iteration
				<a href="https://arxiv.org/abs/1707.02670">[arxiv]</a>.
		<li>June 2017: 
			<span style="font-weight:bold">
			<font color="red">
				New preprint!
			</font>
			</span> An automatic tuner for they hyperparameters of momentum SGD 
				<a href="https://arxiv.org/pdf/1706.03471.pdf">[arxiv]</a>.
		<li>May 2017: Custom scan sequence paper accepted for presentation at ICML 2017!
		<li>April 2017: Invited talk at Workshop on Advances in Computing Architectures, Stanford SystemX
		<li>March 2017: 
		<span style="font-weight:bold">
		<font color="red">
			New preprint!
		</font>
		</span> Custom scan sequences for 
			<a href="https://arxiv.org/pdf/1707.05807.pdf">super fast Gibbs sampling</a>.
		<li>February 2017: Invited to talk at ITA in San Diego.
		<li>February 2017: Spoke at the AAAI 2017 Workshop on Distributed Machine Learning.
		<li>January 2017: Visiting Microsoft Research, Cambridge
		<li>December 2016: At NIPS, presenting our
			<a href="https://arxiv.org/abs/1606.03432">Gibbs sampling paper</a>
		  	dispelling some common beliefs regarding scan orders.
		<li>November 2016: Visiting Microsoft Research New England
		<li>November 2016: Invited talk at SystemX Stanford Alliance Fall Conference 
		<li>November 2016: Full version of asynchrony <a href="papers/asynchrony-begets-momentum.pdf">paper</a>.
	  	<li>September 2016: Talk at Allerton
	  	<li>August 2016: I had the pleasure to give a talk MIT Lincoln Labs.
	  	<li>August 2016: Gave an <a href="{{ site.baseurl}}/asynchrony/">asynchronous optimization</a> talk at Google.
		<li>August 2016: <a href="http://stanford.edu/~imit/tuneyourmomentum/">Blog post</a> on our momentum work.
	  	<li>July 2016: Invited to talk at NVIDIA.
		<li>June 2016: <a href="{{ site.baseurl}}/asynchrony/">Poster</a> at non-convex optimization ICML <a href="http://sites.google.com/site/noncvxicml16/">workshop</a>.
		<li>June 2016: <a href="https://arxiv.org/abs/1606.07365">Poster</a> at OptML 2016 workshop.
		<li>In a recent <a href="http://bit.ly/22wAt0e">note</a>,
		we show that asynchrony in SGD introduces momentum. 
		In the companion <a href="http://bit.ly/ovore">systems paper</a>, we use this
		theory to train deep networks faster.
		  <li>Does periodic model averaging always help? Recent <a href="https://arxiv.org/abs/1606.07365">results</a>.
		<li>Excited to start Postdoc at Stanford University. Will be working with Lester Mackey and Chris R&eacute;.</li>
		<li>Successfully defended my PhD thesis!</li>
		<li>SILO <a href="http://wid.wisc.edu/featured-events/silo032515/">seminar talk</a> at the Wisconsin Institute of Discovery. Loved both Madison and the WID!</li>
		<li>Densest k-Subgraph work <a href="http://devblogs.nvidia.com/parallelforall/gpu-accelerated-graph-analytics-python-numba/">picked up by NVIDIA</a>!</li>
		<li>Our latest <a href="papers/FrogWild.pdf">work</a> has been accepted for presentation at VLDB 2015!</li>
    </ul>
    </div>
    <br>

    <br>

  <h2>Students and postdocs</h2>

<table style="text-align: left;" border="0">
    <tbody>
      <tr>
        <td style="width=50%;vertical-align: top;">
	<a href="https://bradyneal.github.io/aboutme/" target="_blank">
        <img title="Brady Neal", style="width:20%;" alt="Brady Neal" src="images/person-brady.jpg">
	Brady Neal, PhD 
	</a>
	<br>
	<br>
	<a href="https://scholar.google.ca/citations?user=rZvmqJsAAAAJ&hl=en" target="_blank">
        <img title="Adam Ibrahim", style="width:20%;" alt="Adam Ibrahim" src="images/person-adam.jpg">
	Adam Ibrahim, PhD 
	</a>
	<br>
	<br>
        <img title="Remi Piche-Taillefer", style="width:20%;" alt="Remi Piche-Taillefer" src="images/person-missing.png">
	Remi Piche-Taillefer, MSc
	<br>
	<br>
	<a href="https://www.maths.ed.ac.uk/~s1461357/" target="_blank">
        <img title="Nicolas Loizou", style="width:20%;" alt="Nicolas Loizou" src="images/person-nicolas.jpg">
	Nicolas Loizou, Postdoc
	</a>
	<br>
	<br>
	<a href="https://mathemanu.github.io/" target="_blank">
        <img title="Manuela Girotti", style="width:20%;" alt="Manuela Girotti" src="images/person-manuela.jpeg">
	Manuela Girotti, Postdoc
	</a>
	<br>
	<br>
	</td>
        <!--td width=30px></td-->
        <td style="width=50%;vertical-align: top;">
	<a href="https://reyhaneaskari.github.io/" target="_blank">
        <img title="Reyhane Askari-Hemmat", style="width:20%;" alt="Reyhane Askari-Hemmat" src="images/person-reyhane.jpg">
	Reyhane Askari-Hemmat, PhD 
	</a>
	<br>
	<br>
	<a href="https://ajolicoeur.wordpress.com/" target="_blank">
        <img title="Alexia Jolicoeur-Martineau", style="width:20%;" alt="Alexia Jolicoeur-Martineau" src="images/person-alexia.jpg">
	Alexia Jolicoeur-Martineau, PhD 
	</a>
	<br>
	<br>
        <img title="Baptiste Goujeaud", style="width:20%;" alt="Baptiste Goujeaud" src="images/person-missing.png">
	Baptiste Goujeaud, Intern
	<br>
	<br>
	<a href="https://mila.quebec/en/person/amartya-mitra/" target="_blank">
        <img title="Amartya Mitra", style="width:20%;" alt="Amartya Mitra" src="images/person-amartya.jpg">
	Amartya Mitra, Intern
	</a>
	<br>
	<br>
	<a href="https://hiroki11x.github.io" target="_blank">
        <img title="Hiroki Naganuma", style="width:20%;" alt="Hiroki Naganuma" src="images/person-hiroki.png">
	Hiroki Naganuma, PhD
	</a>
	<br>
	<br>

	</td>
	<td style="vertical-align: top;">&nbsp;&nbsp; <br>
        </td>
      </tr>
    </tbody>
  </table>


    <br>


The following students are not supervised by me, but are exceptional collaborators deserving mention.
<br>
<br>
<table style="text-align: left;" border="0">
    <tbody>
      <tr>
        <td style="width=50%;vertical-align: top;">
	Gauthier Gidel
	<br>
	Isabela Albuquerque
	<br>
	Joao Monteiro
	<br>
	Alex Lamb
	</td>
        <td width=30px></td>
        <td style="width=50%;vertical-align: top;">
	</td>
	<td style="vertical-align: top;">&nbsp;&nbsp; <br>
        </td>
      </tr>
    </tbody>
  </table>

  <br>
  <br>


  <h2>Past Students
  (and where they are now) 
</h2>
<table style="text-align: left;" border="0">
    <tbody>
      <tr>
        <td style="width=50%;vertical-align: top;">
	Brady Neal, MSc, Fall 2019 (continuing for PhD)
	<br>
	Seb Arnold, intern, Summer 2018 (PhD candidate at USC)
	<br>
Nicolas Gagne, intern, Summer 2018 (PhD candidate at McGill)
	<br>
Vinayak Tantia, intern, 2018 (Research Engineer at FAIR Montreal)
	</td>
        <td width=30px></td>
        <td style="width=50%;vertical-align: top;">
	</td>
	<td style="vertical-align: top;">&nbsp;&nbsp; <br>
        </td>
      </tr>
    </tbody>
  </table>


<br>
<br>
<h2>Teaching</h2> 

<ul>
	<li> Winter 2021 [planned]:
	 <a href="/ift6085-dl-theory-class-2020/">
			IFT 6085 - Theoretical principles for deep learning
		</a>
	<li>
	<font style="font-weight:bold">
		Fall 2020</font>: <a href="/ift6390-ml-class/">
			IFT 6390 - Fundamentals of Machine Learning 
		</a>
	<li> Winter 2020:
	 <a href="/ift6085-dl-theory-class-2020/">
			IFT 6085 - Theoretical principles for deep learning
		</a>
	<li>Fall 2019: <a href="/ift6390-ml-class/">
			IFT 6390 - Fundamentals of Machine Learning 
		</a>
	<li>Winter 2019: <a href="/ift6085-dl-theory-class-2019/">
			IFT 6085 - Theoretical principles for deep learning
		</a>
	<li>Fall 2018: <a href="/ift6390-ml-class/">
			IFT 6390 - Fundamentals of Machine Learning 
		</a>
	<li>Winter 2018: <a href="/ift6085-dl-theory-class/">
			IFT 6085 - Theoretical principles for deep learning
		</a>
</ul>
			
    <br>

    <br>


  <h2>Publications</h2>
  <!-- {==% bibtex _plugins/style.bst biblio/biblio.bib %} -->
  {% include biblio.html %}

  <br>
  <br>

  <h2>Older publications</h2>
  <!-- {==% bibtex _plugins/style.bst biblio/biblio.bib %} -->
  In another life, I did research in telecommunications as an electrical engineer.
  That experience was my gateway into information theory, optimization and statistics. 
  It also introduced me to the information theory community and some of their unique tools
  for dealing with statistical and learning problems.


  {% include biblio_older.html %}

  <br>

  <h2>Funding acknowledgements</h2>
        <img title="CIFAR", style="width:15%;" alt="CIFAR" src="images/logo-cifar.jpg">
        <img title="Apogee", style="width:20%;" alt="Apogee" src="images/logo-apogee.png">
        <img title="IVADO", style="width:20%;" alt="IVADO" src="images/logo-ivado.png">
        <img title="Samsung", style="width:20%;" alt="Samsung" src="images/logo-samsung.png">
	<br>
        <img title="FRQNT", style="width:18%;" alt="FRQNT" src="images/logo-frqnt.png">
        <img title="NSERC", style="width:18%;" alt="NSERC" src="images/logo-nserc.png">
        <img title="Microsoft Research", style="width:20%;" alt="Microsoft Research" src="images/logo-msr.png">
	<br>
	<br>
	Special thanks to Intel and NVIDIA for donating access to hardware
	and SigOPT for access to their platform for some of our work.
	

<!--   <h1 class="page-heading">Recent News</h1>

  <ul class="post-list">
    {% for post in site.posts %}
      <li>
        <span class="post-meta">{{ post.date | date: "%b %-d, %Y" }}</span>

        <h2>
          <a class="post-link" href="{{ post.url | prepend: site.baseurl }}">{{ post.title }}</a>
        </h2>
      </li>
    {% endfor %}
  </ul> -->

  <!--p class="rss-subscribe">subscribe <a href="{{ "/feed.xml" | prepend: site.baseurl }}">via RSS</a></p-->

</div>
